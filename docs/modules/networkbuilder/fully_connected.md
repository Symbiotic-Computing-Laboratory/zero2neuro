# Fully Connected Neural Network

## Introduction

A fully connected neural network generally translates one vector (the
inputs) into another vector (the outputs).

## Key Components

Three types of layers:  
- Input: Is the layer that takes in data to give to the hidden layer.  
- Hidden: Is where computations happen with linear regression and nonlinear transformations, often thought of as a black box.  
- Output: Is data that the model predicts, there are also nonlinear transformations that occur between the hidden and otuput layers. This can also contain predetermined values while training a model.

Activation Functions:  
There are several activation functions, for more details on the various ones you can use see [Keras Activation Function Documentation](https://keras.io/api/layers/activations/).

### Input Shape

### Hidden Layers

number of untis per layer

non-linearity


### Output Shape

non-linearity

## Regularization

- Dropout
- L1/L2

## Batch Normalization

___

## Example Network Configuration

___

## Oddities

Dropout_input

Output shapes can be multi-dimensional
